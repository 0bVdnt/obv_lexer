# obv_lexer - A Minimal C-like Language Lexer in Rust

obv_lexer is a lexical analyzer (lexer or tokenizer) for a simplified C-like programming language, implemented in Rust. It reads source code from a file (or a default string) and produces a stream of tokens, which are output as a JSON array to standard output. If errors are encountered during lexing, a JSON object describing the error is produced.

This project is designed as a foundational component, potentially for a larger compiler project, and focuses on demonstrating the core principles of lexical analysis.

## Features

- **Tokenizes basic C-like constructs:**
  - Keywords: `int`, `void`, `return`
  - Identifiers: (e.g., `main`, `variableName`, `_foo123`)
  - Integer Constants: (e.g., `0`, `123`, `42`)
  - Punctuation: `(`, `)`, `{`, `}`, `;`
- **Skips Whitespace:** Ignores spaces, tabs, and newlines between tokens.
- **Handles Comments:**
  - Single-line comments: `// ... until end of line`
  - Multi-line comments: `/* ... can span multiple lines ... */` (non-nested)
- **Error Reporting:** Produces structured JSON output for lexical errors, including:
  - `UnexpectedCharacter`: When a character is found that cannot start any known token.
  - `InvalidInteger`: When a numeric literal is malformed or out of range (for `i32`).
  - `NoMatch`: A fallback for when no token rule applies at a position.
- **JSON Output:** Outputs the token stream or error information in JSON format for easy interoperability with other tools or compiler stages written in different languages.
- **Modular Design:** The lexer logic is organized into sub-modules for clarity:

  - `token.rs`: Defines the `Token` enum.
  - `error.rs`: Defines the `LexerError` enum.
  - `core.rs`: Contains the `Lexer` struct and core tokenization logic.
  - `mod.rs`: Aggregates the lexer module and provides its public API.

## Project Structure

```
obv_lexer/
├── Cargo.toml        # Rust project configuration, dependencies
├── src/              # Source code directory
│   ├── main.rs       # Application entry point, handles I/O and orchestrates the lexer
│   └── lexer/          # Lexer module directory
│       ├── mod.rs      # Lexer module entry point, re-exports public items, tests
│       ├── token.rs    # Token enum definition
│       ├── error.rs    # LexerError enum definition
│       └── core.rs     # Lexer struct and core tokenization logic
└── target/           # Build artifacts (generated by `cargo build`)
```

## Building the Project

You need to have the Rust programming language toolchain installed. You can get it from [rustup.rs](https://rustup.rs/).

1.  **Clone the repository (if applicable) or navigate to the project root directory.**

    ```bash
    # If you have cloned a repo:
    # git clone https://github.com/0bVdnt/obv_lexer.git
    # cd obv_lexer
    ```

2.  **Build the project using Cargo:**

    ```bash
    cargo build
    ```

    This command compiles the source code and its dependencies. The executable will be placed in the `target/debug/` directory i.e `target/debug/obv_lexer`.

    For a release build (optimized):

    ```bash
    cargo build --release
    # Executable will be in target/release/
    ```

## Running the Lexer

The lexer can be run from the command line. It accepts an optional file path as an argument. If no file path is provided, it uses a default hardcoded C Program Code.

**Syntax:**

```bash
./target/debug/obv_lexer [path_to_source_file.c]
```

- `[path_to_source_file.c]` is optional.

**Examples:**

1.  **Run with default input:**

    ```bash
    ./target/debug/obv_lexer
    ```

    This will print informational messages about the source code to `stderr` and the JSON token stream (or error) to `stdout`.

    ````bash
      No source file provided. Use default example code.
      --- Source Code ---
      int main () { return 0; }
      -------------------
      ```json
      {
        "Success": [
          "KwInt",
          {
            "Identifier": "main"
          },
          "OpenParen",
          "CloseParen",
          "OpenBrace",
          "KwReturn",
          {
            "Constant": 0
          },
          "Semicolon",
          "CloseBrace"
        ]
      }
    ````

2.  **Run with a specific C source file:**
    Create a file, e.g., `test.c`:

    ```c
    // My test program
    int main(void) {
        /* A simple return */
        return 42;
    }
    ```

    Then run:

    ```bash
    ./target/debug/obv_lexer test.c
    ```

        Output (to `stdout`):

    ```json
    {
      "Success": [
        "KwInt",
        {
          "Identifier": "main"
        },
        "OpenParen",
        "KwVoid",
        "CloseParen",
        "OpenBrace",
        "KwReturn",
        {
          "Constant": 42
        },
        "Semicolon",
        "CloseBrace"
      ]
    }
    ```

3.  **Run with a file containing a lexical error:**
    Create `error.c`:

    ```c
    int main() { return $; }
    ```

    Run:

    ```bash
    ./target/debug/obv_lexer error.c
    ```

    Output (to `stdout`):

    ```json
    {
      "Error": {
        "unexpected_character": {
          "char": "$",
          "pos": 20
        }
      }
    }
    ```

    The program will also exit with a non-zero status code (1) in case of a lexing error.

## How it Works

1.  **Input:** The `main.rs` reads the source code from the specified file or uses a default string.
2.  **Lexer Instantiation:** An instance of `lexer::Lexer` is created with the source code.
3.  **Tokenization (`tokenize_all`):**
    - The `Lexer` iterates through the input string, character by character (tracked by a `position` index).
    - **Skipping:** In each step, it first attempts to skip whitespace and comments (both single-line `//` and multi-line `/* ... */`) using pre-compiled regular expressions.
    - **Token Matching:** If non-skippable characters are found, it tries to match them against a series of regular expressions defined for each token type (punctuation, identifiers, constants) in a specific order.
      - **Identifiers & Keywords:** If a sequence matches the identifier pattern, it's then checked against a predefined list of keywords. If it's a keyword, the corresponding keyword token is produced; otherwise, an identifier token (with its name) is produced.
      - **Constants:** If a sequence matches the integer constant pattern, it's parsed into an `i32` value.
    - **Error Handling:** If, after skipping, the current input does not match any known token pattern, an appropriate `LexerError` is generated (e.g., `UnexpectedCharacter`).
4.  **Output:**
    - If tokenization is successful, a `Vec<Token>` (vector of tokens) is produced.
    - If an error occurs, a `LexerError` is produced.
    - The `main.rs` then serializes this result (either the token list or the error) into a JSON string using the `serde` and `serde_json` crates.
    - The JSON string is printed to standard output.
    - The program exits with status code 0 on success or 1 on lexing error.

## Dependencies

- **`regex`**: For regular expression matching.
- **`lazy_static`**: For efficient one-time initialization of static regexes.
- **`serde`**: For data serialization (specifically `serde::Serialize` derive).
- **`serde_json`**: For serializing the token stream/errors into JSON format.

These are managed by Cargo and listed in `Cargo.toml`.

## Further Development

This lexer serves as a basic starting point. Potential enhancements could include:

- Support for more C data types (float, char).
- String literals and character literals.
- More operators (arithmetic, logical, assignment, etc.).
- Error recovery (attempting to continue lexing after an error).
- Tracking line and column numbers for tokens and errors for better diagnostics.
- Support for preprocessor directives.
- More robust multi-line comment handling (e.g., detecting unterminated comments explicitly).

## License
This project is licensed under the MIT License. See the `LICENSE` file for details.